<!-- timestamp inserted -->
> ğŸ“„ åˆ›å»ºæ—¶é—´ï¼š2025-08-09 02:42:57  
> ğŸ› ï¸ ä¿®æ”¹æ—¶é—´ï¼š2025-08-09 02:42:57


# äº‘æœåŠ¡å™¨ä¸Šä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹å¾®è°ƒä¸éƒ¨ç½²å…¨æµç¨‹å®æˆ˜

## ç›®å½•

1. [é¡¹ç›®ä»‹ç»](#é¡¹ç›®ä»‹ç»)  
2. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)  
3. [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)  
4. [æ¨¡å‹å¾®è°ƒ](#æ¨¡å‹å¾®è°ƒ)  
5. [Gradio å‰ç«¯æ­å»º](#gradio-å‰ç«¯æ­å»º)  
6. [å…¬ç½‘è®¿é—®ä¸éš§é“å·¥å…· ngrok](#å…¬ç½‘è®¿é—®ä¸éš§é“å·¥å…·-ngrok)  
7. [æ€»ç»“ä¸æ‰©å±•](#æ€»ç»“ä¸æ‰©å±•)  
8. [é¡¹ç›®æ–‡ä»¶ç»“æ„](#é¡¹ç›®æ–‡ä»¶ç»“æ„)

---

## é¡¹ç›®ä»‹ç»

æœ¬é¡¹ç›®æ—¨åœ¨æ¼”ç¤ºå¦‚ä½•åœ¨è…¾è®¯äº‘æœåŠ¡å™¨ä¸Šå®Œæˆä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹çš„å¾®è°ƒå’Œåœ¨çº¿éƒ¨ç½²ï¼Œæ¶µç›–ä»æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€æœåŠ¡æ­å»ºåˆ°å…¬ç½‘è®¿é—®çš„å®Œæ•´æŠ€æœ¯æµç¨‹ã€‚  
æ ¸å¿ƒæŠ€æœ¯æ ˆåŒ…æ‹¬ Huggingface Transformersã€Datasetsã€Gradio å’Œ ngrokã€‚

---

## ç¯å¢ƒå‡†å¤‡

- äº‘æœåŠ¡å™¨æ“ä½œç³»ç»Ÿï¼šUbuntu 20.04  
- Python 3.8 åŠä»¥ä¸Šç‰ˆæœ¬  
- ä¾èµ–åŒ…å®‰è£…å‘½ä»¤ï¼š

```bash
apt update && apt install -y python3 python3-pip git 


# å®‰è£… GPU æ”¯æŒçš„æ·±åº¦å­¦ä¹ æ¡†æ¶

# ä»¥ PyTorch ä¸ºä¾‹ï¼ˆCUDA 12.4 å¯¹åº”ç‰ˆæœ¬ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 


pip install transformers datasets accelerate




---

## æ•°æ®å‡†å¤‡


æ–°å»ºä¸€ä¸ª `test_model.py`ï¼š

```python
from transformers import pipeline

# åŠ è½½ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹
classifier = pipeline(
    "sentiment-analysis",
    model="uer/roberta-base-finetuned-jd-binary-chinese",
    device=0  # GPU:0
)

# æµ‹è¯•å‡ å¥è¯
texts = [
    "è¿™ä¸ªäº§å“çœŸçš„å¾ˆå¥½ç”¨ï¼",
    "æˆ‘éå¸¸å¤±æœ›ï¼Œè´¨é‡å¤ªå·®äº†ã€‚",
    "ä¸€èˆ¬èˆ¬ï¼Œæ²¡æœ‰ç‰¹åˆ«æƒŠè‰³çš„åœ°æ–¹ã€‚"
]

for t in texts:
    print(t, "=>", classifier(t))
```

è¿è¡Œï¼š

```bash
python test_model.py
```

ä½ ä¼šçœ‹åˆ°ä¸‰å¥è¯åˆ†åˆ«è¢«æ ‡æ³¨ä¸ºç§¯æï¼ˆpositiveï¼‰æˆ–æ¶ˆæï¼ˆnegativeï¼‰ã€‚

è¿™ä¸ªäº§å“çœŸçš„å¾ˆå¥½ç”¨ï¼ => [{'label': 'positive (stars 4 and 5)', 'score': 0.976917028427124}]
model.safetensors:   5%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                       | 21.0M/409M [00:01<00:21, 18.3MB/s]æˆ‘éå¸¸å¤±æœ›ï¼Œè´¨é‡å¤ªå·®äº†ã€‚ => [{'label': 'negative (stars 1, 2 and 3)', 'score': 0.9843776226043701}]
ä¸€èˆ¬èˆ¬ï¼Œæ²¡æœ‰ç‰¹åˆ«æƒŠè‰³çš„åœ°æ–¹ã€‚ => [{'label': 'negative (stars 1, 2 and 3)', 'score': 0.5168154239654541}]
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409M/409M [00:33<00:00, 12.2MB/s]


















## æ¨¡å‹å¾®è°ƒ

æˆ‘ä»¬è¿›å…¥ **é˜¶æ®µ 3ï¼šå¾®è°ƒæ¨¡å‹**ï¼Œä¸€æ­¥æ­¥æ¥ã€‚


---

## **ğŸ“Œ é˜¶æ®µ 3ï¼šå¾®è°ƒä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹**

### 1. å®‰è£…ä¾èµ–

å…ˆå®‰è£… Hugging Face è®­ç»ƒæ‰€éœ€å·¥å…·ï¼š

```bash
pip install transformers datasets accelerate evaluate
```

---

### 2. åˆ›å»ºè®­ç»ƒè„šæœ¬ `finetune_sentiment.py`
<details>
  <summary>ä»£ç </summary>

```python
from datasets import Dataset
import pandas as pd
import evaluate
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

# 1. è¯»å– CSV å¹¶è½¬æˆ Dataset
df = pd.read_csv("data/chnsenticorp.csv")  # ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®
dataset = Dataset.from_pandas(df)

# 2. æ‹†åˆ†æ•°æ®é›†ï¼ˆ80%è®­ç»ƒï¼Œ20%éªŒè¯ï¼‰
split_dataset = dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]

# 3. åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
model_name = "uer/roberta-base-finetuned-jd-binary-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# 4. å®šä¹‰åˆ†è¯å‡½æ•°
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)

# 5. å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†åšåˆ†è¯
train_dataset = train_dataset.map(preprocess_function, batched=True)
eval_dataset = eval_dataset.map(preprocess_function, batched=True)

# 6. åŠ è½½è¯„ä»·æŒ‡æ ‡
accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return accuracy.compute(predictions=predictions, references=labels)

# 7. è®­ç»ƒå‚æ•°é…ç½®
training_args = TrainingArguments(
    output_dir="./results",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,
    push_to_hub=False,
    load_best_model_at_end=False,
    fp16=True
)

# 8. åˆ›å»º Trainer å®ä¾‹
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# 9. å¼€å§‹è®­ç»ƒ
trainer.train()

# 10. ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
trainer.save_model("./my-chinese-sentiment")
tokenizer.save_pretrained("./my-chinese-sentiment")

print("âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ° ./my-chinese-sentiment")


```

è¿è¡Œï¼š

```bash
python test_my_model.py
```



</details>




æ‰§è¡Œ **python finetune_sentiment.py**æŠ¥é”™

<details>
  <summary>æŠ¥é”™è§£é‡Š</summary>

ä½ è¿™ä¸ªæŠ¥é”™æ˜¯å› ä¸º load_dataset("chnsenticorp") ä¼šå» Hugging Face Hub ä¸‹è½½ä¸­æ–‡æƒ…æ„Ÿæ•°æ®é›†ï¼Œä½†æœåŠ¡å™¨ç°åœ¨è¿ä¸ä¸Š Hugging Faceï¼ˆè¦ä¹ˆæ˜¯ç½‘ç»œè¢«å¢™ï¼Œè¦ä¹ˆæ˜¯æ²¡æœ‰è®¾ç½®è®¿é—® tokenï¼‰

</details>



è§£å†³åŠæ³•

<details>
  <summary>è§£å†³æ–¹æ³•</summary>

---

**æ–¹æ³• 1ï¼šæ‰‹åŠ¨ä¸‹è½½æœ¬åœ°æ•°æ®é›†ï¼ˆæ¨èï¼Œç¨³å®šï¼‰**

1. æˆ‘å¸®ä½ æ‰¾å¥½ `ChnSentiCorp` æ•°æ®é›†çš„ CSV æ–‡ä»¶ï¼ˆåŒ…å«â€œè¯„è®ºâ€å’Œâ€œæ ‡ç­¾â€ï¼‰ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨ `pandas` è¯»æœ¬åœ°æ•°æ®ï¼Œä¸èµ° Hugging Face Hubã€‚
2. ä½ æŠŠæ–‡ä»¶æ”¾åˆ° `/workspace/data/` ä¸‹ï¼Œè„šæœ¬æ”¹æˆï¼š

```python
from datasets import Dataset
import pandas as pd

df = pd.read_csv("data/chnsenticorp.csv")  # å‡è®¾æœ‰ text, label ä¸¤åˆ—
dataset = Dataset.from_pandas(df)
```

è¿™æ ·å®Œå…¨ä¸ç”¨è”ç½‘ã€‚

---

**æ–¹æ³• 2ï¼šé…ç½® Hugging Face å›½å†…é•œåƒ**ï¼ˆå¦‚æœè¦ç»§ç»­ç”¨ Hubï¼‰

```bash
pip install huggingface_hub
huggingface-cli login  # å¦‚æœä½ æœ‰ HF è´¦å·ï¼Œè¾“å…¥ token
```

ç„¶ååœ¨ Python é‡ŒåŠ ï¼š

```python
from datasets import load_dataset
dataset = load_dataset("shibing624/chnsenticorp")
```

`shibing624/chnsenticorp` æ˜¯ç¤¾åŒºä¸Šä¼ çš„ç‰ˆæœ¬ï¼Œæ¯”å®˜æ–¹çš„æ›´å®¹æ˜“ä¸‹è½½ã€‚
ä½†å‰ææ˜¯ä½ çš„æœåŠ¡å™¨èƒ½ç§‘å­¦è®¿é—®ã€‚

---

**æ–¹æ³• 3ï¼šæ¢ä¸€ä¸ªæœ¬åœ°å¯ç”¨çš„ä¸­æ–‡æƒ…æ„Ÿæ•°æ®é›†**
æ¯”å¦‚ç›´æ¥ç”¨å‡ ç™¾æ¡ç¤ºä¾‹æ•°æ®ç»ƒæ‰‹ï¼Œå…ˆè·‘é€šæµç¨‹ï¼Œå†æ¢æ›´å¤§çš„æ•°æ®ã€‚

---

æˆ‘èµ°çš„æ˜¯ **æ–¹æ³• 1ï¼ˆæœ¬åœ°æ•°æ®ï¼‰**ï¼Œè¿™æ ·æˆ‘ä»¬é©¬ä¸Šå°±èƒ½å¼€å§‹å¾®è°ƒï¼Œä¸ç”¨æŠ˜è…¾ç½‘ç»œã€‚

</details>





å°†ä»¥ä¸‹æ•°æ®å†™å…¥`chnsenticorp.csv` æ–‡ä»¶å†…å®¹ï¼Œç„¶åæŠŠå®ƒä¿å­˜åˆ° `/workspace/data/chnsenticorp.csv` å°±å¯ä»¥äº†ã€‚


<details>
  <summary>ç¤ºä¾‹å†…å®¹</summary>

è¿™é‡Œæ˜¯ç¤ºä¾‹å†…å®¹ï¼ŒåŒ…å«å‡ æ¡ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»çš„æ ·æœ¬ï¼Œæ ¼å¼æ˜¯ `text,label`ï¼Œlabel 1 è¡¨ç¤ºç§¯æï¼Œ0 è¡¨ç¤ºæ¶ˆæï¼š

```csv
text,label
è¿™ä¸ªäº§å“çœŸçš„å¾ˆå¥½ç”¨,1
è´¨é‡å¤ªå·®äº†ï¼Œéå¸¸å¤±æœ›,0
æœåŠ¡æ€åº¦éå¸¸å¥½ï¼Œå€¼å¾—æ¨è,1
ä¸å–œæ¬¢è¿™ä¸ªå•†å“ï¼Œä½“éªŒå¾ˆå·®,0
ç‰©ç¾ä»·å»‰ï¼Œè¶…å‡ºé¢„æœŸ,1
åƒåœ¾ï¼Œå®Œå…¨æ²¡æœ‰ç”¨,0
å¿«é€’å¾ˆå¿«ï¼ŒåŒ…è£…ä¹Ÿä¸é”™,1
å·®è¯„ï¼Œæ ¹æœ¬ä¸å€¼è¿™ä¸ªä»·æ ¼,0
ç”¨äº†å‡ å¤©æ‰å‘ç°éå¸¸å¥½ç”¨ï¼Œæ»¡æ„,1
é¢œè‰²å’Œå›¾ç‰‡ä¸€æ ·ï¼Œéå¸¸æ¼‚äº®,1
å®‰è£…å¤æ‚ï¼Œè¯´æ˜ä¹¦ä¸è¯¦ç»†,0
å®¢æœæ€åº¦å†·æ·¡ï¼Œä¸æƒ³ä¹°äº†,0
æ€§ä»·æ¯”é«˜ï¼Œä¸‹æ¬¡è¿˜ä¼šä¹°,1
ä¸œè¥¿åäº†ï¼Œé€€è´§éº»çƒ¦,0
ç‰©æµæ…¢å¾—è®©äººå¿ƒç„¦,0
ä½“éªŒå¾ˆå¥½ï¼Œæ“ä½œç®€å•,1
è®¾è®¡æ—¶å°šï¼Œæ‰‹æ„Ÿä¸é”™,1
æ²¡æœ‰é¢„æœŸçš„å¥½ï¼Œå¤±æœ›,0
åŒ…è£…ç ´æŸï¼Œå½±å“å¿ƒæƒ…,0
ä»·æ ¼åˆç†ï¼Œåˆ’ç®—,1
äº§å“æè´¨å¾ˆå·®ï¼Œæ„Ÿè§‰ä¾¿å®œè´§,0
å®¢æœè€å¿ƒè§£ç­”ï¼Œè§£å†³äº†é—®é¢˜,1
æ”¶åˆ°è´§åå¾ˆæ»¡æ„ï¼Œéå¸¸æ£’,1
å¿«é€’å‘˜æ€åº¦å·®ï¼Œå½±å“å¿ƒæƒ…,0
è´¨é‡éå¸¸å¥½ï¼Œè¶…å‡ºé¢„æœŸ,1
äº§å“æœ‰å¼‚å‘³ï¼Œä¸å»ºè®®è´­ä¹°,0
ä½¿ç”¨æ–¹ä¾¿ï¼Œæ•ˆæœæ˜æ˜¾,1
é€€è´§è¿‡ç¨‹ç¹çï¼Œæµªè´¹æ—¶é—´,0
éå¸¸å–œæ¬¢ï¼Œæ¨èè´­ä¹°,1
åŠŸèƒ½é½å…¨ï¼Œæ“ä½œæµç•…,1

```

</details>

æ‰§è¡Œ

âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ° ./my-chinese-sentiment




åŠ è½½åˆšè®­ç»ƒå¥½çš„æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œç»™ä¸€å¥ä¸­æ–‡æ–‡æœ¬åšæƒ…æ„Ÿé¢„æµ‹ï¼š

<details>
  <summary>ä»£ç ç¤ºä¾‹</summary>

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np

# 1. åŠ è½½è®­ç»ƒåä¿å­˜çš„æ¨¡å‹å’Œåˆ†è¯å™¨
model_dir = "./my-chinese-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForSequenceClassification.from_pretrained(model_dir)

# 2. å‡†å¤‡è¾“å…¥æ–‡æœ¬
text = "è¿™ä¸ªäº§å“çœŸçš„å¾ˆå¥½ç”¨ï¼Œæˆ‘å¾ˆå–œæ¬¢ï¼"

# 3. åˆ†è¯ç¼–ç 
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=128)

# 4. æ¨¡å‹æ¨ç†
model.eval()
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

# 5. å–æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ç´¢å¼•
predicted_class_id = torch.argmax(logits, dim=1).item()

# 6. è¾“å‡ºç»“æœï¼ˆ0=æ¶ˆæï¼Œ1=ç§¯æï¼‰
labels_map = {0: "æ¶ˆæ", 1: "ç§¯æ"}
print(f"æ–‡æœ¬ï¼š{text}")
print(f"é¢„æµ‹æƒ…æ„Ÿï¼š{labels_map[predicted_class_id]}")
```

---

æŠŠè¿™æ®µä»£ç ä¿å­˜æˆæ¯”å¦‚ `predict.py`ï¼Œæ‰§è¡Œï¼š

```bash
python predict.py
```

å°±èƒ½çœ‹åˆ°æ¨¡å‹å¯¹ç¤ºä¾‹æ–‡æœ¬çš„æƒ…æ„Ÿé¢„æµ‹ç»“æœäº†ã€‚

(base) root@VM-14-154-ubuntu:/workspace# python predict.py
æ–‡æœ¬ï¼šè¿™ä¸ªäº§å“çœŸçš„å¾ˆå¥½ç”¨ï¼Œæˆ‘å¾ˆå–œæ¬¢ï¼
é¢„æµ‹æƒ…æ„Ÿï¼šç§¯æ
---


</details>



















### å¾®è°ƒæ–¹æ¡ˆ

<details>
  <summary>æ–¹æ¡ˆ</summary>
å› ä¸ºç”¨çš„æ˜¯ Hugging Face Transformers è®­ç»ƒçš„ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹ï¼Œæƒ³ç”¨ç±»ä¼¼ **LLaMA-Factory** çš„å¸¦ Web UI çš„å·¥å…·æ¥å¾®è°ƒå®ƒï¼Œä»¥ä¸‹æ˜¯å¯è¡Œæ–¹æ¡ˆå’Œæ€è·¯ï¼š

---

## 1. LLaMA-Factory UI æ˜¯ä¸“é—¨é’ˆå¯¹ LLaMA ç³»åˆ—å¤§è¯­è¨€æ¨¡å‹è®¾è®¡çš„

* å®ƒé›†æˆäº†è®­ç»ƒã€æ¨ç†çš„ Web ç•Œé¢
* æ”¯æŒ LoRA / QLoRA å¾®è°ƒç­‰
* ä¸»è¦é¢å‘ç”Ÿæˆä»»åŠ¡å’Œå¤§æ¨¡å‹ï¼Œä¸ç›´æ¥æ”¯æŒä¸€èˆ¬ Transformers åˆ†ç±»æ¨¡å‹å¾®è°ƒ

---

## 2. ä½ çš„æ¨¡å‹ï¼ˆåŸºäº RoBERTaï¼‰å¾®è°ƒ + UI æ–¹æ¡ˆ

### æ–¹æ¡ˆ Aï¼šç”¨ Gradio æˆ– Streamlit è‡ªå·±æ­å»ºç®€å•çš„ Web UI

* **Gradio** æ˜¯å¾ˆæµè¡Œçš„ Python åº“ï¼Œå¯ä»¥ç”¨å‡ è¡Œä»£ç å¿«é€Ÿåšä¸€ä¸ªç½‘é¡µç•Œé¢ï¼Œæ—¢èƒ½æ¨ç†ä¹Ÿèƒ½è§¦å‘è®­ç»ƒ
* ä½ å¯ä»¥æŠŠè®­ç»ƒè„šæœ¬æ”¹æˆå‡½æ•°ï¼Œå‰ç«¯ä¼ å…¥è®­ç»ƒå‚æ•°å’Œæ•°æ®ï¼Œåå°è°ƒç”¨ Hugging Face Trainer è®­ç»ƒï¼Œæ¨ç†éƒ¨åˆ†ç”¨ä½ å·²ä¿å­˜æ¨¡å‹åŠ è½½åšé¢„æµ‹

ç¤ºä¾‹ï¼šç®€æ˜“æ¨ç† + è®­ç»ƒæŒ‰é’®ç•Œé¢ï¼ˆå¯æ‰©å±•ï¼‰

```python
import gradio as gr
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
import pandas as pd

model_dir = "./my-chinese-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForSequenceClassification.from_pretrained(model_dir)

def predict(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    outputs = model(**inputs)
    probs = torch.softmax(outputs.logits, dim=-1)
    label = torch.argmax(probs).item()
    return "ç§¯æ" if label == 1 else "æ¶ˆæ"

def train(data_csv):
    # è¿™é‡Œç®€å•ç¤ºèŒƒï¼šæ¥æ”¶ CSV æ–‡ä»¶è·¯å¾„ï¼Œè¿è¡Œè®­ç»ƒè„šæœ¬æˆ–è°ƒç”¨ Trainer
    df = pd.read_csv(data_csv.name)
    # ä½ å¯ä»¥å†™å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼Œå‚è€ƒä¹‹å‰ä»£ç 
    return "è®­ç»ƒå·²å®Œæˆï¼"

with gr.Blocks() as demo:
    gr.Markdown("# ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»å¾®è°ƒä¸é¢„æµ‹")
    text_input = gr.Textbox(label="è¾“å…¥æ–‡æœ¬é¢„æµ‹æƒ…æ„Ÿ")
    pred_button = gr.Button("é¢„æµ‹")
    pred_output = gr.Label()
    
    pred_button.click(predict, inputs=text_input, outputs=pred_output)
    
    file_input = gr.File(label="ä¸Šä¼ è®­ç»ƒæ•°æ® CSV")
    train_button = gr.Button("å¼€å§‹è®­ç»ƒ")
    train_output = gr.Textbox()
    
    train_button.click(train, inputs=file_input, outputs=train_output)

demo.launch()
```

ç”¨ `python your_gradio_app.py` å¯åŠ¨åå°±æœ‰ä¸ªç®€å• UI å¯ä»¥è¯•ã€‚

---

### æ–¹æ¡ˆ Bï¼šç”¨å·²æœ‰çš„ Hugging Face Spaces

* Hugging Face Spaces æ”¯æŒ Gradio/Streamlitï¼Œå¯ä»¥å¿«é€Ÿéƒ¨ç½²åœ¨çº¿ Web åº”ç”¨
* ä½†é€šå¸¸ä¸æ”¯æŒåœ¨çº¿è®­ç»ƒï¼Œåªé€‚åˆæ¨ç†

---

### æ–¹æ¡ˆ Cï¼šä½¿ç”¨ä¸“ä¸š MLOps / AutoML å¹³å°

* æ¯”å¦‚ Azure ML Studioã€Weights & Biasesã€SageMaker ç­‰
* æœ‰å®Œæ•´ç•Œé¢ç®¡ç†è®­ç»ƒã€æ¨ç†å’Œç›‘æ§ï¼Œä½†é…ç½®å’Œæˆæœ¬è¾ƒé«˜

---

## 3. æ€»ç»“

* **LLaMA-Factory çš„ Web UI** æ˜¯ä¸“é—¨ç»™å¤§è¯­è¨€æ¨¡å‹ç”¨çš„ï¼Œä¸èƒ½ç›´æ¥æ‹¿æ¥è®­ç»ƒ RoBERTa åˆ†ç±»æ¨¡å‹
* å¯ä»¥ç”¨ **Gradio** è‡ªå·±å¿«é€Ÿæ­ä¸ªç•Œé¢ï¼Œå®ç°ç®€å•çš„è®­ç»ƒå’Œæ¨ç†äº¤äº’
* è®­ç»ƒéƒ¨åˆ†é€»è¾‘å®Œå…¨å¯å¤ç”¨å·²æœ‰çš„ä»£ç ï¼ŒUI åªè´Ÿè´£æ–‡ä»¶ä¸Šä¼ å’Œå‚æ•°ä¼ é€’
---
</details>






## gradio-å‰ç«¯æ­å»º

æˆ‘é‡‡ç”¨æ–¹æ¡ˆAï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªåŸºäº **Gradio** çš„ç®€å•ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»å¾®è°ƒ+æ¨ç† Web UI ç¤ºä¾‹ã€‚è¿™ä¸ªç•Œé¢æ”¯æŒï¼š

* è¾“å…¥æ–‡æœ¬åšæƒ…æ„Ÿé¢„æµ‹
* ä¸Šä¼ æ–°çš„è®­ç»ƒæ•°æ® CSVï¼Œç‚¹å‡»æŒ‰é’®è§¦å‘å¾®è°ƒè®­ç»ƒ

---

<details>
  <summary>ç¤ºèŒƒ</summary>

### å®Œæ•´ç¤ºèŒƒä»£ç ï¼ˆ`app.py`ï¼‰

```python
import gradio as gr
import pandas as pd
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import evaluate

# åˆå§‹åŠ è½½å·²æœ‰æ¨¡å‹å’Œåˆ†è¯å™¨
model_dir = "./my-chinese-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForSequenceClassification.from_pretrained(model_dir)

# è¯„ä»·æŒ‡æ ‡
accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return accuracy.compute(predictions=predictions, references=labels)

# é¢„æµ‹å‡½æ•°
def predict(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1)
        label = torch.argmax(probs).item()
    return "ç§¯æ" if label == 1 else "æ¶ˆæ"

# è®­ç»ƒå‡½æ•°
def train(file_obj):
    try:
        df = pd.read_csv(file_obj.name)
        if "text" not in df.columns or "label" not in df.columns:
            return "CSVå¿…é¡»åŒ…å«'text'å’Œ'label'ä¸¤åˆ—"
        
        dataset = Dataset.from_pandas(df)
        split_ds = dataset.train_test_split(test_size=0.2, seed=42)
        train_ds = split_ds["train"]
        eval_ds = split_ds["test"]

        def preprocess_fn(examples):
            return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)

        train_ds = train_ds.map(preprocess_fn, batched=True)
        eval_ds = eval_ds.map(preprocess_fn, batched=True)

        training_args = TrainingArguments(
            output_dir="./results",
            save_strategy="epoch",
            learning_rate=2e-5,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=16,
            num_train_epochs=2,
            weight_decay=0.01,
            logging_dir="./logs",
            logging_steps=50,
            push_to_hub=False,
            load_best_model_at_end=False,
            fp16=True
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_ds,
            eval_dataset=eval_ds,
            tokenizer=tokenizer,
            compute_metrics=compute_metrics
        )

        trainer.train()

        # è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹ï¼Œæ–¹ä¾¿åç»­é¢„æµ‹ç”¨æœ€æ–°æ¨¡å‹
        trainer.save_model(model_dir)
        tokenizer.save_pretrained(model_dir)

        return "âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²æ›´æ–°"
    except Exception as e:
        return f"è®­ç»ƒå¤±è´¥ï¼Œé”™è¯¯ï¼š{e}"

# Gradio UI å¸ƒå±€
with gr.Blocks() as demo:
    gr.Markdown("# ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»å¾®è°ƒä¸é¢„æµ‹")
    
    with gr.Tab("é¢„æµ‹"):
        text_input = gr.Textbox(label="è¾“å…¥æ–‡æœ¬")
        pred_button = gr.Button("é¢„æµ‹æƒ…æ„Ÿ")
        pred_output = gr.Label()
        pred_button.click(predict, inputs=text_input, outputs=pred_output)
    
    with gr.Tab("å¾®è°ƒè®­ç»ƒ"):
        file_input = gr.File(label="ä¸Šä¼ è®­ç»ƒæ•°æ® CSVï¼ˆå« text, label åˆ—ï¼‰", file_types=[".csv"])
        train_button = gr.Button("å¼€å§‹å¾®è°ƒè®­ç»ƒ")
        train_output = gr.Textbox()
        train_button.click(train, inputs=file_input, outputs=train_output)

demo.launch(server_name="0.0.0.0", server_port=7860)

```

---

### ä½¿ç”¨è¯´æ˜

1. **å‡†å¤‡å¥½ä½ ä¹‹å‰è®­ç»ƒå¥½çš„æ¨¡å‹**ï¼Œè·¯å¾„æ˜¯ `./my-chinese-sentiment`ã€‚
2. å®‰è£…ä¾èµ–ï¼š

```bash
pip install gradio transformers datasets evaluate torch pandas
```

3. è¿è¡Œï¼š

```bash
python app.py
```

4. æµè§ˆå™¨æ‰“å¼€è‡ªåŠ¨å¼¹å‡ºçš„é“¾æ¥æˆ–è®¿é—® `http://127.0.0.1:7860`ï¼Œä½ å°±èƒ½çœ‹åˆ°ç•Œé¢ã€‚

5. **é¢„æµ‹é¡µ**ï¼šè¾“å…¥ä¸­æ–‡æ–‡æœ¬ç‚¹å‡»â€œé¢„æµ‹æƒ…æ„Ÿâ€ã€‚

6. **å¾®è°ƒè®­ç»ƒé¡µ**ï¼šä¸Šä¼ åŒ…å« `text` å’Œ `label` çš„ CSV æ–‡ä»¶ï¼Œç‚¹å‡»â€œå¼€å§‹å¾®è°ƒè®­ç»ƒâ€ï¼Œè®­ç»ƒç»“æŸåæ¨¡å‹ä¼šè‡ªåŠ¨æ›´æ–°ã€‚

---

### æ³¨æ„

* è®­ç»ƒä¼šä½¿ç”¨ä½ çš„å½“å‰ç¡¬ä»¶ï¼ˆæœ€å¥½æœ‰ GPUï¼‰ã€‚
* è®­ç»ƒæ—¶é—´è§†æ•°æ®é‡å’Œæœºå™¨æ€§èƒ½è€Œå®šã€‚
* è®­ç»ƒè¿‡ç¨‹çš„æ—¥å¿—ä¼šè¾“å‡ºåˆ°æ§åˆ¶å°ã€‚
* è¿™ä¸ªç•Œé¢ç¤ºä¾‹ç®€å•ç›´è§‚ï¼Œä½ å¯ä»¥æ ¹æ®éœ€æ±‚è‡ªè¡Œæ‰©å±•ã€‚

---
å®‰è£… sudoã€‚
ç¡®ä¿æœåŠ¡å™¨æœ¬èº«æ²¡æœ‰é¢å¤–é˜²ç«å¢™é™åˆ¶
è¿è¡Œå‘½ä»¤æ£€æŸ¥ï¼ˆUbuntu å¸¸è§æ˜¯ ufwï¼‰ï¼š


---

## Ubuntu/Debian ç³»ç»Ÿå®‰è£… ufw

```bash
apt update
apt install ufw
```

---


è¿è¡Œ
```bash
ufw status
```

å¦‚æœæç¤º `command not found`ï¼Œ`Status: inactive`è¯´æ˜ç³»ç»Ÿæ²¡å®‰è£… `ufw`ã€‚


1. ç”¨éš§é“å·¥å…·ï¼ˆæ¨èï¼‰
é€šè¿‡éš§é“å·¥å…·æš´éœ² Gradio ç«¯å£ï¼Œå®ç°æœ¬åœ°æµè§ˆå™¨è®¿é—®ã€‚

å¸¸ç”¨å·¥å…·
ngrokï¼ˆéœ€æ³¨å†Œï¼‰

cloudflaredï¼ˆCloudflare æä¾›ï¼‰

frpï¼ˆéœ€è¦ä½ è‡ªå·±çš„å…¬ç½‘æœåŠ¡å™¨åšè·³æ¿ï¼‰

2. ngrok ç®€å•ä½¿ç”¨ç¤ºèŒƒ
ä¸‹è½½å¹¶å®‰è£… ngrokï¼š

---

## ä¸€ã€å‡†å¤‡å·¥ä½œ

1. æ³¨å†Œ [ngrok å®˜ç½‘](https://ngrok.com/) å¹¶ç™»å½•ï¼Œè¿›å…¥ Dashboard è·å–ä½ çš„ **Authtoken**ã€‚

2. äº‘IDE é‡Œä¸‹è½½å¹¶è§£å‹ ngrokï¼š

```bash
wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
unzip ngrok-stable-linux-amd64.zip
chmod +x ngrok
./ngrok version
```



---

## æ­£ç¡®ä¸‹è½½æœ€æ–° ngrok ç‰ˆæœ¬çš„æ–¹æ³•

ngrok æœ€æ–°ç‰ˆæœ¬æ˜¯ 3.xï¼Œå®˜æ–¹æœ€æ–° Linux 64ä½ä¸‹è½½åœ°å€ï¼š

```bash
wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz -O ngrok.tgz
tar -xzf ngrok.tgz
chmod +x ngrok
./ngrok version
```





---

æ‰§è¡Œå®Œåï¼Œè®¤è¯ token ï¼š

```bash
./ngrok authtoken <ä½ çš„token>
```


## å…¬ç½‘è®¿é—®ä¸éš§é“å·¥å…·-ngrok
---
### æ‰§è¡Œ

1. å…ˆè¿è¡Œä½ çš„ Gradio æœåŠ¡ï¼š

```bash
python app.py
```

ç¡®è®¤å®ƒåœ¨è¿è¡Œï¼Œå¹¶ä¸”è¾“å‡ºç±»ä¼¼ï¼š

```
Running on local URL:  http://0.0.0.0:7860
```

2. åœ¨å¦ä¸€ä¸ªç»ˆç«¯çª—å£ï¼Œå¯åŠ¨ ngrok éš§é“ï¼š

```bash
./ngrok http 7860
```

3. å¤åˆ¶ ngrok æ˜¾ç¤ºçš„å…¬ç½‘åœ°å€ï¼Œç”¨æµè§ˆå™¨è®¿é—®ï¼Œå°±èƒ½çœ‹åˆ°ä½ çš„ Gradio é¡µé¢ã€‚

---ã€‚






## æ€»ç»“ä¸æ‰©å±•

* æœ¬æ–‡æ¼”ç¤ºäº†ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹çš„å®Œæ•´å¾®è°ƒä¸éƒ¨ç½²æµç¨‹
* ä»‹ç»äº†å¦‚ä½•è§£å†³äº‘æœåŠ¡å™¨ç«¯å£è®¿é—®é™åˆ¶é—®é¢˜
* åç»­å¯å°è¯• Docker å®¹å™¨åŒ–éƒ¨ç½²ã€æ­å»ºæ›´å¤æ‚çš„åœ¨çº¿æœåŠ¡æ¥å£
* ngrok å…è´¹è´¦å·çš„å…¬ç½‘åœ°å€æ˜¯ä¸´æ—¶éšæœºçš„ï¼Œé‡å¯åä¼šå˜åŒ–ã€‚
* å»ºè®®åªç”¨äºè°ƒè¯•ã€ä¸´æ—¶è®¿é—®ã€‚

---

## é¡¹ç›®æ–‡ä»¶ç»“æ„

```
/workspace/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ chnsenticorp.csv       # ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»æ•°æ®CSV
â”œâ”€â”€ my-chinese-sentiment/      # è®­ç»ƒåæ¨¡å‹æ–‡ä»¶å¤¹
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ app.py                     # Gradio å¯åŠ¨è„šæœ¬
â”œâ”€â”€ finetune_sentiment.py      # å¾®è°ƒè®­ç»ƒè„šæœ¬
â”œâ”€â”€ ngrok                      # ngrok å¯æ‰§è¡Œæ–‡ä»¶
â”œâ”€â”€ ngrok.tgz                  # ngrok å‹ç¼©åŒ…
â””â”€â”€ start_gradio_with_ngrok.py # ï¼ˆå¯é€‰ï¼‰è‡ªåŠ¨å¯åŠ¨Gradioå’Œngrokè„šæœ¬
```

---

